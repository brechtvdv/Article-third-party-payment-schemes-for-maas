<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta charset="utf-8" />
  <title>Comparing a polling and push-based approach for live Open Data interfaces</title>
  <link rel="stylesheet" media="screen" href="styles/screen.css" />
  <link rel="stylesheet" media="print"  href="styles/print.css" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" media="all"    href="styles/katex.css" />
  <meta name="citation_title" content="Comparing a polling and push-based approach for live Open Data interfaces">
  <meta name="citation_author" content="Brecht Van de Vyvere" />
  <meta name="citation_author" content="Pieter Colpaert" />
  <meta name="citation_author" content="Ruben Verborgh" />
  
  <meta name="citation_publication_date" content="2020/03/18" />
</head>

<body prefix="rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns# rdfs: http://www.w3.org/2000/01/rdf-schema# owl: http://www.w3.org/2002/07/owl# xsd: http://www.w3.org/2001/XMLSchema# dcterms: http://purl.org/dc/terms/ dctypes: http://purl.org/dc/dcmitype/ foaf: http://xmlns.com/foaf/0.1/ v: http://www.w3.org/2006/vcard/ns# pimspace: http://www.w3.org/ns/pim/space# cc: https://creativecommons.org/ns# skos: http://www.w3.org/2004/02/skos/core# prov: http://www.w3.org/ns/prov# qb: http://purl.org/linked-data/cube# schema: http://schema.org/ void: http://rdfs.org/ns/void# rsa: http://www.w3.org/ns/auth/rsa# cert: http://www.w3.org/ns/auth/cert# cal: http://www.w3.org/2002/12/cal/ical# wgs: http://www.w3.org/2003/01/geo/wgs84_pos# org: http://www.w3.org/ns/org# biblio: http://purl.org/net/biblio# bibo: http://purl.org/ontology/bibo/ book: http://purl.org/NET/book/vocab# ov: http://open.vocab.org/terms/ sioc: http://rdfs.org/sioc/ns# doap: http://usefulinc.com/ns/doap# dbr: http://dbpedia.org/resource/ dbp: http://dbpedia.org/property/ sio: http://semanticscience.org/resource/ opmw: http://www.opmw.org/ontology/ deo: http://purl.org/spar/deo/ doco: http://purl.org/spar/doco/ cito: http://purl.org/spar/cito/ fabio: http://purl.org/spar/fabio/ oa: http://www.w3.org/ns/oa# as: https://www.w3.org/ns/activitystreams# ldp: http://www.w3.org/ns/ldp# solid: http://www.w3.org/ns/solid/terms# acl: http://www.w3.org/ns/auth/acl# dio: https://w3id.org/dio# as: https://www.w3.org/ns/activitystreams# oa: http://www.w3.org/ns/oa# ldp: http://www.w3.org/ns/ldp#" typeof="schema:CreativeWork sioc:Post prov:Entity">
  <header>
  <h1 id="comparing-a-polling-and-push-based-approach-for-live-open-data-interfaces">Comparing a polling and push-based approach for live Open Data interfaces</h1>

  <ul id="authors">
    <li><a rev="lsc:participatesIn" property="foaf:maker schema:creator schema:author schema:publisher" href="https://brechtvdv.github.io/" typeof="foaf:Person schema:Person" resource="https://w3id.org/people/brechtvdv/#me">Brecht Van de Vyvere</a></li>
    <li><a rev="lsc:participatesIn" property="foaf:maker schema:creator schema:author schema:publisher" href="https://pietercolpaert.be" typeof="foaf:Person schema:Person" resource="https://pietercolpaert.be/#me">Pieter Colpaert</a></li>
    <li><a rev="lsc:participatesIn" property="foaf:maker schema:creator schema:author schema:publisher" href="https://ruben.verborgh.org/" typeof="foaf:Person schema:Person" resource="https://ruben.verborgh.org/profile/#me">Ruben Verborgh</a></li>
  </ul>

  <ul id="affiliations">
    <li id="idlab">IDLab,
          Department of Electronics and Information Systems,
          Ghent University – imec</li>
  </ul>

  <section class="context">
    <h2 id="in-reply-to">In reply to</h2>
    <ul>
      <li><a href="https://icwe2020.webengineering.org" rel="as:inReplyTo">Call for papers 2020, International Conference On Web Engineering</a></li>
    </ul>
  </section>

</header>

<!-- Hack to make our custom fonts load in print-mode -->
<!-- https://stackoverflow.com/questions/39364259/chrome-print-preview-doesnt-load-media-only-print-font-face -->
<p><span class="printfont1"> </span>
<span class="printfont2"> </span>
<span class="printfont3"> </span>
<span class="printfont4"> </span></p>

<div class="double-column">

 <section id="abstract" inlist="" rel="schema:hasPart" resource="#abstract">
<div datatype="rdf:HTML" property="schema:description">
      <h2 property="schema:name">Abstract</h2>
      <p>There are two mechanisms for publishing live changing resources on the Web: a client can pull the latest state of a resource or the server pushes updates to the client. In the state of the art, it is clear that pushing delivers a lower latency compared to pulling, however, this has not been tested for an Open Data usage scenario where 15k clients are not an exception. Also, there are no general guidelines when to use a polling or push-based approach for publishing live changing resources on the Web. We performed (i) a field report of live Open datasets on the European and U.S. Open Data portal and (ii) a benchmark between HTTP polling and Server-Sent Events (SSE) under a load of 25k clients. In this article, we compare the scalability and latency of updates on the client between polling and pushing. For the scenario where users want to receive an update as fast as possible, we found that SSE excels above polling in three aspects: lower CPU usage on the server, lower latency on the client and more than double the number of clients that can be served. However, considering that users can perceive a certain maximum latency on the client (MAL) of an update acceptable, we describe in this article at which MAL point a polling interface can be able to serve a higher number of clients than pushing. Open Data publishers can use these insights to determine which mechanism is the most cost-effective for the usage scenario they foresee of their live updating resources on the Web.</p>

      <p><span class="printonly firstpagefooter">
<span class="footnotecopyright">
This is a print-version of an article first written for the Web. The Web-version is available at https://brechtvdv.github.io/Article-Live-Open-Data-interfaces .                            <br />
Copyright © 2020 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).
</span>
</span></p>
    </div>
</section>


<!-- https://www.rubensworks.net/raw/publications/2016/Continuous_Client-Side_Query_Evaluation_over_Dynamic_Linked_Data.pdf -->

<main>
  <!-- Add sections by specifying their file name, excluding the '.md' suffix. -->
 <section id="introduction" inlist="" rel="schema:hasPart" resource="#introduction">
<div datatype="rdf:HTML" property="schema:description">
        <h2 property="schema:name">Introduction</h2>

        <p>The Open Data deployment scheme <span class="references">[<a href="#ref-1">1</a>]</span> defines 5 steps that data publishers can undertake to raise the technical and semantical interoperability of their Open datasets on the Web. With the use of the Hypertext Transfer Protocol (HTTP) as a communication protocol, a dataset becomes technically interoperable with the Web of data <span class="references">[<a href="#ref-2">2</a>, <a href="#ref-3">3</a>]</span>. This allows Open Data consumers to retrieve a resource (e.g. a document that is part of the dataset) by sending an HTTP GET method to the Uniform Resource Identifier (URI) of the resource. 
<span class="placeholder printonly">
<span style="display: block; height: 7em;"></span>
<!-- This is a dummy placeholder for the LNCS first page footnote -->
</span>
For live changing resources, such as the measurements of a sensor, there are two communication mechanisms to share an update in a timely fashion to clients. First, there is pull where the client initiates the action to retrieve a resource. This category has two representatives: HTTP polling and HTTP long polling. Next, you have push where the server pushes updates of a resource to the client. Server-Sent Events (SSE) and Websockets are implementations for this mechanism. 
Lubbers et al. <span class="references">[<a href="#ref-4">4</a>]</span> compared Websockets with HTTP polling for a dataset that updates every second and up to 100k clients, but this was only a theoretical analysis of the bandwidth usage and latency. Depending on the size of the header information, a bandwidth reduction of 500:1 can be made with Websockets and a latency reduction of 3:1. Pimentel et al. <span class="references">[<a href="#ref-5">5</a>]</span> went a step further and investigated how the physical distance between publisher and consumer impacts the overall latency. They performed a comparison between polling, long polling and Websockets where a new sensor update of roughly 100 bytes is published per second. They defined formulas to check when polling or long polling is feasible for updates on time, dependent on the network latency. When the network latency exceeds half the update rate of the dataset, then Websockets is the better choice. However, there is no evaluation performed on the performance of the server under a high load of clients, which is an important factor that needs to be considered for Open Data publishing.</p>

        <p>One of the key features of publishing data on the Web is HTTP caching, which has not been addressed in related work <span class="references">[<a href="#ref-4">4</a>, <a href="#ref-5">5</a>, <a href="#ref-6">6</a>]</span>. This allows a resource to become stateless and can be shared with proxy caches or Content Network Delivery (CDN) services to offload the server. With push-based interfaces like Websockets, caching of a resource is not possible as the server needs to actively push the content in a stateful manner to all subscribed clients. In previous work <span class="references">[<a href="#ref-7">7</a>]</span>, a minimum set of technical requirements and a benchmark between pull (HTTP polling) and push (Websockets) have been introduced for publishing live changing resources on the Web. This benchmark tested with only 200 clients, which did not yield conclusive results on the latency or scalability issues that arise inside an Open Data ecosystem, where client numbers of 15 000 are not an exception <span class="references">[<a href="#ref-8">8</a>]</span>. In this article, we will run a similar benchmark between HTTP polling and SSE with up to 25k clients. SSE is tested instead of Websockets, because it has a similar performance <span class="references">[<a href="#ref-6">6</a>]</span>, communication is unidirectional which is suitable for Open Data and lastly, it only relies on HTTP instead of a Websockets protocol, which lowers the complexity of reusing a dataset.</p>

        <p>The remainder of this article is structured as follows: we will provide in the related work section an overview of publication techniques and the current state of publishing RDF Streams on the Web. RDF Streams are applicable for describing live updating resources with the Resource Description Framework (RDF) and thus resolve the fourth step of the Open Data deployment scheme <span class="references">[<a href="#ref-1">1</a>]</span>. Thereafter, we conduct a field report to quantify how many live Open datasets are available on the Open Data portals of Europe and the U.S. to observe which update retrieval mechanism is used in different domains. In the problem statement section, we define our research questions and hypotheses which we will then evaluate with a benchmark between HTTP polling and SSE. In the discussion and conclusion, based on the results of the benchmark, we will propose some guidelines for data publishers when to use pull or push interfaces.</p>

      </div>
</section>

 <section id="related_work" inlist="" rel="schema:hasPart" resource="#related_work">
<div datatype="rdf:HTML" property="schema:description">
        <h2 property="schema:name">Related work</h2>

        <h3 id="web-publication-protocols">Web publication protocols</h3>

        <p><strong>HTTP polling</strong> A client sends an HTTP GET request to the server, waits for a certain time interval after retrieving the response and starts again with requesting the resource. The benefit of this approach is that a resource becomes stateless and HTTP caching is possible. However, there is no strict guideline on how clients should time their request. For variantly updating resources, a client can’t predict when the next update will be available. A higher polling frequency can minimize the latency on the client, but this comes at a higher bandwidth cost <span class="references">[<a href="#ref-4">4</a>]</span>.</p>

        <p><strong>HTTP long polling</strong> With long polling, the server only returns a response when a new update is available. This way, a client does not send redundant requests like HTTP polling. Also, the client does not wait before sending a request again. A resource becomes stateful as the server needs to maintain all the connections open. Pimental et al. <span class="references">[<a href="#ref-5">5</a>]</span> showed that long polling can have a similar performance as Websockets when the underlying network latency is lower than half the data measurement rate.</p>

        <p><strong>Server-Sent Events</strong> With the growing demand for (near) realtime applications, HTTP is extended in 2014 with the support of Server-Sent Events (SSE). Similarly to long polling, the server holds the connection open for every client, but this remains open for pushing multiple updates instead of one. With the use of the EventSource API (supported by all browsers except IE and Edge), clients can receive updates of a resource in an event-driven fashion. Using SSE over HTTP/1.1 has the disadvantage that every requested resource requires a separate TCP connection, which can run into the limited number of connections a browser can open per domain. However, this is solved for servers that support HTTP/2, which multiplex all requests and responses over one connection.</p>

        <p><strong>Websockets</strong> The Websocket protocol provides a bidirectional communication channel over one TCP connection for every client. HTTP is used to set up a handshake between client and server for transmitting data, but further communication happens over a raw TCP connection. The WHATWG Websocket standard describes how messages can be pushed between client and server, but there are also sub protocols (MQTT <span class="references">[<a href="#ref-9">9</a>]</span>, CoAP, etc.) for more advanced features, for example a publish/subscribe broker to receive or send updates of a specific resource. Websockets has a similar performance as SSE <span class="references">[<a href="#ref-6">6</a>]</span>, but a lower transmission latency when the server needs to send large messages above 7.5 kilobytes. Also, for client to server communication provides Websockets a lower transmission latency <span class="references">[<a href="#ref-6">6</a>]</span> than using HTTP.</p>

        <p><strong>WebSub</strong> The WebSub <span class="references">[<a href="#ref-10">10</a>]</span> specification extends the communication pattern between clients and servers from above protocols with a third actor <em>hub</em>. A resource can be retrieved from the publisher (server), but consumers can also subscribe for updates through a hub instead of polling the resource URL. A resource is coupled with one topic, which is exposed by one or more hubs for fault tolerance. Reusers of the data can receive updates by setting up a Web accessible server and subscribing to a topic. Hubs then send updates through HTTP POST requests (Webhook mechanism) to this server. While Open Data publishers can benefit from distributed hubs in an Open Data ecosystem, for example a hub can be reused by multiple data publishers, Open Data reusers are required to deploy a Web server to receive updates they are interested in. To enable the use case of autonomous intelligent agents <span class="references">[<a href="#ref-11">11</a>]</span> that wish to retrieve updates of a resource through a topic, then a service for subscribing to a topic must be made available. As this service will also need to decide on exposing polling or pushing to agents, we will not further elaborate on WebSub in this article.</p>

        <h3 id="rdf-streams">RDF Streams</h3>

        <p>Arasu et al. <span class="references">[<a href="#ref-12">12</a>]</span> defines a stream S as a (possibly infinite) bag (multiset) of elements〈s, τ〉where s is a tuple (the actual data without the timestamp of the element) belonging to the schema of S and τ ∈ T is the timestamp of the element. The Resource Description Framework (RDF) Stream Processing (RSP) Community Group, which focuses on processing RDF-modelled data, has applied this definition for RDF streams <span class="references">[<a href="#ref-13">13</a>]</span> where an RDF stream S is a (potentially) unbounded sequence of timestamped RDF statements in non-decreasing time order. TripleWave <span class="references">[<a href="#ref-14">14</a>]</span> is a tool that transforms Web streams, which only differs from an RDF stream by its data model <span class="references">[<a href="#ref-7">7</a>]</span>, into RDF streams and republishes them with a polling and/or push-based (Websockets, MQTT) interface. These streams can be consumed by other RDF stream processors (RSP) for continuous query answering with SPARQL-based query models (C-SPARQL <span class="references">[<a href="#ref-15">15</a>]</span>, CQLS <span class="references">[<a href="#ref-16">16</a>]</span>, TPF Query Streamer <span class="references">[<a href="#ref-17">17</a>]</span>). Dell’Aglio et al. <span class="references">[<a href="#ref-18">18</a>]</span> describes for the publication of an RDF stream that both push-based and polling interfaces can be supported; the consumer may choose what it prefers. Also, several requirements <span class="references">[<a href="#ref-19">19</a>]</span> are defined for RSP query engines of which requirement 6 “timely fashion” acknowledges <span class="references">[<a href="#ref-18">18</a>]</span> that the timing of results depend on the application scenario and thus the requirements of the consumer of the data publication or query service. In the next section, we will look into how the timely fashion requirement is applied for live Open datasets.</p>

        <p><!-- While RDF allows data publishers to describe knowledge in triple-based statements, it is important to notice that a non-RDF Web stream of a live dataset only really differs from an RDF stream by its data model <span class="references">\[<a href="#ref-7">7</a>\]</span> and thus, we will look into how RDF streams are currently published on the Web. --></p>
      </div>
</section>

  <section id="fieldreport" inlist="" rel="schema:hasPart" resource="#fieldreport">
<div datatype="rdf:HTML" property="schema:description">
        <h2 property="schema:name">Field report on live Open datasets</h2>

        <p>This section gives an overview of how many live Open datasets are available in the <a href="https://www.europeandataportal.eu/data/datasets">European</a> and <a href="https://data.gov">U.S.</a> Open Data portals, what the rate of publication is and which update mechanism is used. Datasets were retrieved by doing a full-text search on “real-time”. Only working and up-to-date datasets are mentioned. Note that this is a non-exhaustive overview, because among other reasons not all Open datasets are harvested by these portals. We also briefly describe the update mechanisms that are used in the public transport and cryptocurrency trading domains.</p>

        <figure id="report" class="table">

          <table>
            <thead>
              <tr>
                <th>Country</th>
                <th>Datasets</th>
                <th>Update interval</th>
                <th>Update mechanism</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Belgium</td>
                <td>Vehicles position (Public transport MIVB)</td>
                <td>20s</td>
                <td>Polling</td>
              </tr>
              <tr>
                <td>Belgium</td>
                <td>Bicycle counter</td>
                <td>realtime</td>
                <td>Polling</td>
              </tr>
              <tr>
                <td>Belgium</td>
                <td>Park+rides</td>
                <td>realtime</td>
                <td>Polling</td>
              </tr>
              <tr>
                <td>France</td>
                <td>Parking and bicycle stations availability</td>
                <td>60s</td>
                <td>Polling</td>
              </tr>
              <tr>
                <td>Sweden</td>
                <td>Notifications about Lightning Strikes</td>
                <td>realtime</td>
                <td>Push-based</td>
              </tr>
              <tr>
                <td>Ireland</td>
                <td>weather station information, the Irish National Tide Gauge Network</td>
                <td>3600s</td>
                <td>Polling</td>
              </tr>
              <tr>
                <td>UK</td>
                <td>River level data</td>
                <td>900s</td>
                <td>Polling</td>
              </tr>
              <tr>
                <td>UK</td>
                <td>Cycle hire availability &amp; arrival predictions (Transport for London Unified API)</td>
                <td>300s</td>
                <td>Polling</td>
              </tr>
              <tr>
                <td>UK</td>
                <td>Arrival predictions (Transport for London Unified API)</td>
                <td>realtime</td>
                <td>Push-based</td>
              </tr>
              <tr>
                <td>U.S.</td>
                <td>Real-Time Traffic Incident Reports of Austin-Travis County</td>
                <td>300s</td>
                <td>Polling</td>
              </tr>
              <tr>
                <td>U.S.</td>
                <td>True Time API (arrival information and location of public transport vehicles)</td>
                <td>realtime</td>
                <td>Polling</td>
              </tr>
              <tr>
                <td>U.S.</td>
                <td>Current Bike Availability by Station (Nextbike)</td>
                <td>300s</td>
                <td>Polling</td>
              </tr>
              <tr>
                <td>U.S.</td>
                <td>USGS Streamflow Stations</td>
                <td>24h</td>
                <td>Polling</td>
              </tr>
              <tr>
                <td>U.S.</td>
                <td>NOAA water level (tidal) data of 205 Stations for the Coastal United States and Other Non-U.S. Sites</td>
                <td>360s</td>
                <td>Polling</td>
              </tr>
              <tr>
                <td>U.S.</td>
                <td>National Renewable Energy Laboratory <span class="references">[<a href="#ref-20">20</a>]</span></td>
                <td>60s</td>
                <td>Polling</td>
              </tr>
              <tr>
                <td>U.S.</td>
                <td>RTC MetStation real time data</td>
                <td>360s</td>
                <td>Polling</td>
              </tr>
              <tr>
                <td>U.S.</td>
                <td>Seattle Real Time Fire 911 Calls</td>
                <td>300s</td>
                <td>Polling</td>
              </tr>
            </tbody>
          </table>

          <figcaption>
            <p><span class="label">Table 1:</span> Overview of live Open datasets according to their country, how fast it updates and whether a polling or push interface is used.</p>
          </figcaption>
        </figure>

        <p>On <a href="#report">Table 1</a> we see that live Open datasets from only five countries are harvested by the European Open Data portal. While we can argue that there are more relevant datasets than on this overview, for example by browsing for Open Data portals of specific cities or using Google Dataset Search <span class="references">[<a href="#ref-21">21</a>]</span>, we still get a broad view of the current state-of-the-art interfaces. Only 2 datasets have a push interface available, both using the Websocket protocol, of which only the Transport for London (TfL) API from the U.K. is free to use. Interestingly, the True Time API from the U.S. offers the same functionality as the TfL API with arrival predictions for public transport, but uses polling instead of push-based update mechanism. On the one hand, we found live datasets related to the environment (water level, weather, etc.) that publish at a lower rate (from every minute to every hour) with a polling-based interface. On the other hand, we found mobility related datasets whose update interval fits between realtime (as fast as possible) and 5 minutes.</p>

        <p>We also examined the public transport domain where GTFS-RT, a data specification for publishing live transit updates, takes no position <span class="references">[<a href="#ref-22">22</a>]</span> on how updates should be published, except that HTTP should be used. OpenTripPlanner (OTP), a world-wide used multi-modal route planner which allows retrieving GTFS-RT updates and bicycle availabilities, also supports both approaches: setup a frequency in seconds for polling or subscribe to a push-based API.</p>

        <p>When people want to trade money or digital coins, it is crucial that the latency of price tickings, books, etc. are as low as possible, otherwise it can literally cost them money. The rate of publication of these live datasets are also typically below 1 second. Therefore, publishers in the cryptocurrency trading domain heavily use push-based mechanisms for their clients. This however does not mean that a HTTP polling approach is not used. Websockets are de-facto used as a bidirectional communication channel is required for trading. We tested three publishers (Bitfinex, Bitmex and gdax) and saw over a span of one year that they were still available which makes us believe that Open push-based interfaces are a viable option for other domains as well.</p>

      </div>
</section>

  <section id="problemstatement" inlist="" rel="schema:hasPart" resource="#problemstatement">
<div datatype="rdf:HTML" property="schema:description">
        <h2 property="schema:name">Problem Statement</h2>

        <p>In the field report, we saw that there is no strict guideline whether to use polling or pushing for a certain dataset. Based on the insights from the field report and related work, we define the following research question:</p>

        <p>Research question: Which kind of Web interface for server to client communication is best suited for publishing live Open Data in function of server-side cost, scalability and latency on the client?</p>

        <p>Following hypotheses are defined which will be answered in the discussion:</p>

        <p><strong>H1</strong>: Using a Server-Sent Events interface will result in a lower latency on the client, compared to a polling interface.</p>

        <p><strong>H2</strong>: The server-side CPU cost of an HTTP polling interface is initially higher than a Server-Sent Events interface, but increases less steeply when the number of clients increases.</p>

        <p><strong>H3</strong>: From a certain number of clients onward, the server cost of a Server-Sent Events interface exceeds the server cost of a polling interface.</p>

      </div>
</section>

  <section id="benchmark" inlist="" rel="schema:hasPart" resource="#benchmark">
<div datatype="rdf:HTML" property="schema:description">
        <h2 property="schema:name">Benchmark HTTP polling versus Server-Sent Events</h2>

        <h3 id="evaluation-design">Evaluation design</h3>

        <p><strong>Update interval of live dataset</strong> The experiments focus on observing the latency on the client when a server needs to serve a high number of clients. As we want to observe the latency on the client per update and we expect a higher latency when the server works under a high load of clients, it is important to reserve enough time between updates. <a href="#report">Table 1</a> shows that most datasets have an update interval in the range of seconds. By choosing a fixed update interval of 5 seconds for the live dataset in this experiment, there should be enough time to observe the latency on the client between two updates and still have a representative update interval according to <a href="#report">Table 1</a>. Also, an invariantly changing dataset allows to set HTTP caching headers according to the update interval, which is an opportune circumstance for HTTP polling.</p>

        <p><strong>Live dataset</strong> A JSON object with a size of 5.2 kB is generated every 5s, which has a similar size as the park+rides dataset (5.6 kB) from <a href="#report">Table 1</a>. This object is annotated with a timestamp that indicates when this object was generated and is used by clients to calculate the latency on the client. Furthermore, it is published inside a HTTP document for clients that use HTTP polling or it is directly pushed to clients with SSE.</p>

        <p><strong>Latency on the client</strong> The goal of this benchmark is to observe the time between the generation of an update and when a client can further process it. We define this as the latency on the client of an update. For HTTP polling, this depends on timing its request as closely as possible after a new update is available. Based on the caching headers of a response (Cache-Control for HTTP/1.1 or Expires for HTTP/1.0), a client could calculate the optimal time for its next request. In this benchmark, we choose to continuously fetch the HTTP document with a pause of 500ms between the previous response and the next request, because we expect a similar polling implementation by Open Data reusers like OpenTripPlanner.</p>

        <p><strong>Web API</strong> The live data is published with a server written in the Node.js Web application framework Express and exposes 2 API routes: /polling to retrieve a JSON document containing the latest value with an HTTP GET request and /sse to receive updates through an open TCP connection with Server-Sent-Events. The latter is naively implemented server-side with a for loop that pushes updates to every client. Multiple optimizations are possible (multi-threading, load balancing, etc.), but to make a fair comparison between HTTP polling and SSE we focus on having a single-threaded implementation for both approaches. By only using a for loop, all work needs to be done by the default Node.js single-threaded event loop. For HTTP polling, we use nginx as reverse proxy and enable single threading by configuring the number of worker_processes to 1. In order that nginx can handle many simultaneous connections with clients, the number of worker_connections is set to 10k.</p>

        <p><strong>HTTP caching</strong> Two HTTP caching components are available: one is implemented server-side using the HTTP cache of nginx, the other one at the client-side. When a client fetches the document containing the most recent update, it will first check if a non-expired copy is available in its cache (<a href="#sequencediagram-caching">Fig. 1</a>). Web browsers have this feature enabled by default, but the Node.js clients in this benchmark need to use the <a href="https://www.npmjs.com/package/cacheable-request">cacheable-request</a> NPM package to support HTTP caching. An unexpected side effect of using nginx (version 1.17.7) is that it does not dynamically update the max-age value in the Cache-Control header when returning a copy from its cache. This means that a cached copy with a time-to-live of 1s will still have a max-age of 5s which leads into extra client-side caching for 5s instead of 1s. To circumvent this behaviour in our benchmark, we also added the Expires header, which indicates when the document is expired. This requires that the clocks of the server and clients are synchronized which is the case for the testbed we used. For future work, we suggest to use Varnish as reverse proxy, which dynamically updates the Cache-Control header. In <a href="#sequencediagram-caching">Fig. 1</a>, we see that the client makes a request to nginx when its cache is expired. When nginx’s cache is also expired, then  only the first request will be let through (proxy_cache_lock on) to retrieve the document from the back-end server over a persistent keep-alive connection that is configured. Other requests need to wait until nginx received the response and then pull from the updated cache. The max-age value is calculated by subtracting the time that is already passed (the current time - the time the last update is generated) from the frequency a new update is generated (5000ms). The Expires header is calculated by adding the update frequency to the time of the last update. Finally, nginx removes the Cache-Control header, which obliges the client to use the Expires header for the correct timing of its cache.</p>

        <figure id="sequencediagram-caching" style="display:flex; flex-wrap:wrap">
<center>
<img src="img/Retrieving latest data with HTTP polling.png" width="399px" style="min-width: 50%; flex:1; border: none; box-shadow: none;" />
</center>
<figcaption>
            <p><span class="label">Fig. 1:</span> Two cache components (client-side and server-side/nginx) are used for HTTP polling. As Nginx does not dynamically update the max-age value from the Cache-Control header, we fall back on the Expires header for client-side caching.</p>
          </figcaption>
</figure>

        <p><strong>High number of clients</strong> A benchmark environment is created using the Cloudlab testbed at the University of Utah, which had at the time of writing the biggest number (200+) among alternatives available to us of bare metal servers. This is necessary for our envisaged scenario where we need to deploy thousands of Web clients to simulate the impact on an Open Data interface. 200 HP ProLiant <a property="schema:citation http://purl.org/spar/cito/cites" href="https://docs.cloudlab.us/hardware.html">m400</a> <span class="references">[<a href="#ref-23">23</a>]</span> servers are used for our benchmark, each containing a CPU architecture with eight 64-bit ARMv8 (Atlas/A57) cores at 2.4 GHz, 64 GB of RAM, 120 TB of SATA flash storage. Notice that a m400 server uses ARM which is generally lower in performance than traditional x86 server architectures which could lead to faster detection of performance losses. Lastly, we use the Kubernetes framework to easily orchestrate the deployment and scaling of our server and clients that are containerized with Docker.</p>

        <p><strong>Logging results</strong> A time series database (InfluxDB) is deployed where clients log their latency on the client. Also, the visualisation tool Grafana is deployed to monitor whether all clients are initialized and polling or subscribed as expected and then to export the results as CSV. When an update is received on the client, only 10% is randomly logged to InfluxDB to prevent an excess of updates. We exported several minutes of recordings of the latency on the client per test, which we deem enough, for evaluation. To log the usage of the server and client (CPU and memory), we use the Kubernetes <a href="https://github.com/kubernetes-incubator/metrics-server.git">Metrics Server</a>. Similarly to retrieving the resource usage of a Linux machine with the ‘top’ command, we can use ‘kubectl top pods’ to extract the resource usage of Kubernetes pods. For each test, we ran this multiple times and calculated the average for plotting. The Node.js back-end and nginx reverse proxy are deployed in one single pod. This means we can easily monitor the overall resource usage for HTTP polling from both components together.</p>

        <h3 id="results">Results</h3>

        <p>The results from our benchmark are split into two parts: first, we will show the latency on the client with density charts. Then we will look into the resource usage. We will first test HTTP polling without using nginx. This way, we demonstrate the performance boost nginx creates.</p>

        <p><strong>Polling without nginx</strong> On <a href="#latency-polling-without-nginx">Fig. 2</a>, we can see a group of density charts for polling without using nginx. The y-axis represents the number of clients (100, 1000, etc.) that are deployed in polling mode, while the x-axis represents the time in ms it took to retrieve a new update. For every number of clients, there is a separate density chart showing the distribution of latencies on the client that are measured. A client still uses a client-side cache and polls every 500ms, but it directly contacts the server Web API when its cache expires. For 100 clients, more than half of the updates is retrieved below 0.5s. Up to 2000 clients, the majority of updates are retrieved beneath one second. Above 2000 clients, the server struggles to respond efficiently as the latency on the client is spread from 0s up to 5s. We were unable to deploy more than 5000 clients, because the server fails to handle the number of requests.</p>

        <figure id="latency-polling-without-nginx">
<center>
<img style="max-width:40%; height:auto;" src="img/polling_without_cache.png" />
</center>
<figcaption>
            <p><span class="label">Fig. 2:</span> Latency on the client with polling without using nginx. The server is able to answer effectively up to 2000 clients and becomes unstable above 5000 clients.</p>
          </figcaption>
</figure>

        <p><strong>Polling</strong> With nginx added to the server as a reverse proxy with HTTP caching enabled, we can see on <a href="#latency-polling-pubsub">Fig. 3.1</a> that the server is able to handle 8000 clients instead of 5000 clients and have a similar latency for 100 and 1000 clients as without nginx (<a href="#latency-polling-without-nginx">Fig. 2</a>). From 2000 clients on, a peak of the latency on the client appears between 1s and 2s. Also, a peak exists between 3s and 4s starting from 4000 clients. At 8000 clients, the distribution is evenly spread between 0s and 2s. Clients have a polling frequency of 500ms and start polling at different times, which is one of the causes of this spread. In addition, all requests wait until the cache is updated and then nginx returns responses single-threaded. To see whether this is not caused by our client implementation, we performed a benchmark with the wrk HTTP benchmarking tool, which generates a significant amount of requests to test the HTTP response latency instead of the latency on the client to retrieve an update. A wrk benchmark was performed for 30 seconds, using 12 threads, keeping 400 HTTP connections open and timeout for response times above 4s. Wrk measured a maximum response latency of 3.89s with 13 responses timed out above 4s and could reach 2.52k requests/s which acknowledges insights from the density charts on <a href="#latency-polling-pubsub">Fig. 3.1</a>.</p>

        <figure style="display:flex; flex-wrap:wrap" id="latency-polling-pubsub">
<img src="img/polling.png" width="399px" style="min-width: 50%; flex:1; border: none; box-shadow: none;" />
<img src="img/pubsub_with_sleep.png" width="399px" style="min-width: 50%; flex:1; border: none; box-shadow: none;" />
<figcaption>
            <p><span class="label">Fig. 3:</span> Latency on the client with polling (Fig. 3.1) and Server-Sent Events (Fig. 3.2). Polling scales up to 8k clients, while Server-Sent Events can serve 25k clients.</p>
          </figcaption>
</figure>

        <p><strong>Server-Sent Events</strong> On <a href="#latency-polling-pubsub">Fig. 3.2</a> we can see that the maximal latency on the client with a SSE interface increases with the number of clients. For 5k clients, the latency on the client is still below a second, but for 25k clients this is evenly distributed between 0s and 4s. During implementation, we faced a kernel buffer issue where data transmission is queued until no data was written from Node.js to the HTTP response objects from clients. This caused an increasing minimal latency on the client and also a higher maximal latency on the client up to 1.5s for 20k clients. A continuous data transmission was achieved by running a sleep function of 1ms per 1000 clients, because we saw on the density charts of <a href="#latency-polling-pubsub">Fig. 3.2</a> that SSE could respond efficiently up to 1000 clients.</p>

        <p><strong>Resource usage</strong> We measured the CPU and memory usage for the three above-mentioned approaches. The CPU metric of a Kubernetes pod, in which our server resides, is measured in mCPUs (milliCPUs). 1000 mCPUs are equivalent to 1 AWS vCPU or 1 Hyperthread on a bare-metal Intel processor with <a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/">Hyperthreading</a>. Memory usage is measured in mebibytes <a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#meaning-of-memory">(MiB)</a>. On <a href="#server-cost">Fig. 4.1</a>, we can see that polling without nginx has the steepest curve for CPU and that SSE still has a significant CPU advantage over polling with nginx. We believe this is caused by SSE having less overhead than polling, although nginx is able to minimize this with connections that are kept alive, gzip compression and caching. For 5000 clients, we see that nginx decreases CPU usage by half compared to polling without nginx. The memory usage stabilizes for polling (<a href="#server-cost">Fig. 4.2</a>) with preference for polling with nginx. For SSE, this continuously increases with the number of clients as every client connection is held in memory.</p>

        <figure style="display:flex; flex-wrap:wrap" id="server-cost">

<img src="img/cpu.png" width="399px" style="min-width: 50%; flex:1; border: none; box-shadow: none;" />

<img src="img/memory.png" width="399px" style="min-width: 50%; flex:1; border: none; box-shadow: none;" />

<figcaption><span class="label">Fig. 4:</span> 
	CPU and memory usage of polling and Server-Sent Events.
	Left figure (Fig. 4.1), we see that SSE uses less milliCPU than polling.
	Right figure (Fig. 4.2): polling has a low memory footprint (&lt;100 MiB), while SSE needs to keep every client connection continuously in memory.
</figcaption>
</figure>
      </div>
</section>

  <section id="discussion" inlist="" rel="schema:hasPart" resource="#discussion">
<div datatype="rdf:HTML" property="schema:description">
        <h2 property="schema:name">Discussion</h2>

        <p>Based on the benchmarking results from previous section, we start our discussion by verifying our hypotheses. Thereafter, we will answer our research questions more in depth.</p>

        <p>Our first hypothesis H1 states that a SSE interface will result in a lower latency on the client than with polling. When comparing the latency the client on <a href="#latency-polling-pubsub">Fig. 3</a>, we see that SSE always has a lower maximal latency on the client than a polling interface with server-side cache enabled, so we accept H1. Surprisingly, to achieve this distribution for SSE from 0s onwards, we had to add a sleep function of 1ms between every 1000 clients. Otherwise, all responses are first stored in the kernel buffer before getting transmitted which causes a higher minimum latency on the client. This behavior is not described in implementation guidelines of SSE or Websockets so we hope that this article can help informing the Node.js community.</p>

        <p>For our second hypothesis H2, we expected a faster growing server cost of SSE than polling. Previously <span class="references">[<a href="#ref-7">7</a>]</span>, it was argued that the capabilities of HTTP caching would outperform the server cost of SSE for a high number of clients, although polling has an initial higher server cost. CPU usage results (<a href="#server-cost">Fig. 4</a>) show that nginx indeed improves the CPU usage for polling, but it is still steeper than SSE, so we reject H2. Publishers should take note of the higher memory footprint of SSE, because all client connections are saved in memory. When data needs to be encrypted using TLS, then we expect that the server-side CPU cost for both polling and pushing will be only be slighty higher, because TCP connections are kept alive for both approaches and thus the time and resource expensive TLS handshake only needs to be done once for the first HTTP request. <span class="references">[<a href="#ref-24">24</a>]</span>.</p>

        <p>Hypothesis H3 can also be falsified, because the CPU usage of polling grows apart higher than SSE for a large number of clients. In other words, answering requests with a cached copy still has a higher CPU cost than pushing updates directly. In terms of scalability, we saw in <a href="#latency-polling-pubsub">Fig. 3</a> that SSE is able to serve 25k clients, while polling could only serve 8k clients. From our hypotheses, we see that SSE is favoured based on scalability, server cost and latency on the client.</p>

        <p>The field report (<a href="#fieldreport">Section 3</a>) shows for the vehicle positions dataset that updates are generated every 20s and polling is used. When an end user wants to reuse this information inside the OpenTripPlanner application, then a polling frequency (s) needs to be specified. This depends on the maximal latency on the client that an end user perceives as acceptable. Even if the update interval of the live dataset is known, e.g. every 5s like in our benchmark, matching the polling frequency with this update interval would still create a latency on the client distribution between 0s and the polling frequency. In the worst case, an HTTP response is returned just before a new update arrives, so the client will only fetch this update with the next request round. Given the results of our hypotheses, we question at which point the maximum acceptable latency on the client (MAL) of an end user must be in order that our polling interface can serve more clients than SSE. In the next paragraph, we will describe how we can theoretically calculate this MAL cut-off point between polling and SSE.</p>

        <p>In our benchmark, we tested with the Wrk HTTP benchmarking tool that our polling interface could serve up to 2.52k requests/s. When all users would have started polling at different starting times every 5s, so they configured a MAL of 5s, then our polling interface could have served theoretically up to 12.6k clients (2.52*5) instead of 8k from our benchmark. This can be generalized with the following formula, which states that the maximum supported number of requests/s of a polling interface (2.52k in our case) must be higher than the expected number of users that make one request every MAL seconds:</p>

        <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mi>e</mi><mi>q</mi><mi>u</mi><mi>e</mi><mi>s</mi><mi>t</mi><msub><mi>s</mi><mrow><mi>m</mi><mi>a</mi><mi>x</mi><mi>s</mi><mi>u</mi><mi>p</mi><mi>p</mi><mi>o</mi><mi>r</mi><mi>t</mi><mi>e</mi><mi>d</mi></mrow></msub><mi mathvariant="normal">/</mi><mi>s</mi><mo>&#x2265;</mo><mi>U</mi><mi>s</mi><mi>e</mi><mi>r</mi><mi>s</mi><mo>&#x2217;</mo><mi>r</mi><mi>e</mi><mi>q</mi><mi>u</mi><mi>e</mi><mi>s</mi><mi>t</mi><mi mathvariant="normal">/</mi><mi>M</mi><mi>A</mi><mi>L</mi><mtext>&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;&#xa0;</mtext><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Requests_{max supported}/s  &#x2265;  Users * request/MAL\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mord mathdefault">u</span><span class="mord mathdefault">e</span><span class="mord mathdefault">s</span><span class="mord mathdefault">t</span><span class="mord"><span class="mord mathdefault">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">x</span><span class="mord mathdefault mtight">s</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">p</span><span class="mord mathdefault mtight">p</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight">d</span></span></span></span></span><span class="vlist-s">&#x200b;</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mord">/</span><span class="mord mathdefault">s</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&#x2265;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">U</span><span class="mord mathdefault">s</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">s</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">&#x2217;</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="mord mathdefault">u</span><span class="mord mathdefault">e</span><span class="mord mathdefault">s</span><span class="mord mathdefault">t</span><span class="mord">/</span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord mathdefault">A</span><span class="mord mathdefault">L</span><span class="mspace">&#xa0;</span><span class="mspace">&#xa0;</span><span class="mspace">&#xa0;</span><span class="mspace">&#xa0;</span><span class="mspace">&#xa0;</span><span class="mspace">&#xa0;</span><span class="mspace">&#xa0;</span><span class="mspace">&#xa0;</span><span class="mspace">&#xa0;</span><span class="mspace">&#xa0;</span><span class="mspace">&#xa0;</span><span class="mspace">&#xa0;</span><span class="mspace">&#xa0;</span><span class="mspace">&#xa0;</span><span class="mspace">&#xa0;</span><span class="mspace">&#xa0;</span><span class="mspace">&#xa0;</span><span class="mspace">&#xa0;</span><span class="mspace">&#xa0;</span><span class="mspace">&#xa0;</span><span class="mspace">&#xa0;</span><span class="mspace">&#xa0;</span><span class="mspace">&#xa0;</span><span class="mopen">(</span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span>

        <p>Open Data publishers can calculate with formula (1) how many users with a certain MAL can be	 maximally served. In practice, users can configure a higher polling frequency than the expected MAL, so the number of users that can be served will probably be lower. To compare this with SSE, we see on <a href="#latency-polling-pubsub">Fig. 3.2</a> that SSE can maximally serve 25k clients over a MAL of 4s, which is still more than polling can serve (10k clients) with this MAL. By increasing the MAL, we found that a MAL of 10s allows our polling interface to serve the same number of clients (25k) as SSE. This should not be interpreted as a universal number, because there are other factors that can influence this number: an AMD CPU architecture could be more performant than the ARM architecture we used or publishing in a global network could add extra network latency for polling <span class="references">[<a href="#ref-5">5</a>]</span>. Nonetheless, this number gives an indication that users must have a relatively high MAL (&gt; 10s) in order that polling can serve a higher number of clients than pushing with the same amount of resources. This brings us to our research question where our single-threaded comparison shows that pushing is the best choice up to a maximum latency on the client of 10s. For datasets where all users configure a MAL above 10s, then a polling interface is capable to serve a higher number of clients, which can be calculated with formula (1).</p>

        <p><a href="#server-cost">Fig. 4</a> showed that HTTP caching is crucial for a polling interface to lower the CPU usage, but for variantly updating datasets it is not possible to foresee when the next update will happen. For this use case, we advise to only cache the response for 1s in the reverse proxy (micro caching), and thus still off-load the back-end Web API. At last, caching headers should always be set if possible, according to the arrival of the next update so the user can still configure its preferred MAL and bandwidth usage can be reduced.</p>

      </div>
</section>

  <section id="conclusion" inlist="" rel="schema:hasPart" resource="#conclusion">
<div datatype="rdf:HTML" property="schema:description">
        <h2 property="schema:name">Conclusion</h2>

        <p>In <a href="#related_work">related work</a> of the RDF Stream Processing Community Group <span class="references">[<a href="#ref-18">18</a>]</span> and the field report on live Open datasets (<a href="#fieldreport">Section 3</a>), we saw that publishing live changing resources on the Web leaves the options for polling and push-based mechanisms open. With this article, we shed some light into this topic by running a benchmark between a Server-Sent Event and polling interface. In contrast with traditional HTTP benchmarks, we focused on assessing the latency on the client of an update instead of the HTTP response latency. We extend existing work <span class="references">[<a href="#ref-5">5</a>]</span>, because we saw that a push mechanism is also the best option when the server needs to handle a high number of clients. If the latency on the client must be as low as possible, then the server CPU cost of HTTP polling with caching enabled does not outperform pushing <span class="references">[<a href="#ref-7">7</a>]</span>. Data publishers can use our results, which reflect the performance of a pull and push mechanism over a single thread, to foresee when to scale their infrastructure in function of the number of clients and the expected maximum latency on the client. The application scenario that users have a maximal acceptable latency on the client (MAL) of at least 10s makes polling more scalable than pushing, although this is a theoretical number. Configuring the MAL is a task that an Open Data reuser should be able to choose and this cannot be forced by the data publisher by setting a caching header. Because of this, caching headers should always be applied for invariant streams, but its timing should not be further than the next update. For variantly updating streams where the timing of the next update is unknown, we advice to use micro caching on the reverse proxy. At last, Open Data publishers should do user research (conduct a survey or investigate query logs <span class="references">[<a href="#ref-25">25</a>]</span>) to find out which MAL is most likely to be used for each dataset and verify if their current infrastructure is fit for this by applying formula (1). For example, the vehicle position dataset from the field report in <a href="#fieldreport">Section 3</a> has a new update available every 20s. If users also configure their polling frequency in function of this interval, so their MAL is above 10s, then polling is the preferred interface based on the number of clients that can be served.</p>

        <p>The “timely fashion” requirement is currently only applied for each component individually (from Web stream to RDF stream and RSP query engines). In future work, we would like to investigate how this requirement can be resolved from a true user perspective, such as Smart City Dashboards, and how this requirement goes top-down to all the underlying components.</p>

      </div>
</section>

</main>

<footer>
  <section id="acknowledgements" inlist="" rel="schema:hasPart" resource="#acknowledgements">
<div datatype="rdf:HTML" property="schema:description">
        <h2 property="schema:name">Acknowledgements</h2>

        <p>We would like to express our gratitude to <a href="https://twitter.com/rafke">Raf Buyle</a> and <a href="https://twitter.com/psbonte">Pieter Bonte</a> for their support during the writing process of this article.</p>
      </div>
</section>

<section>
<h2 id="references">References</h2>
<dl class="references">
  <dt id="ref-1">[1]</dt>
  <dd resource="https://5stardata.info/en/" typeof="schema:CreativeWork">Berners-Lee, T.: 5 Star Data. <a href="https://5stardata.info/en/">https:/​/​5stardata.info/en/</a> (2009).</dd>
  <dt id="ref-2">[2]</dt>
  <dd resource="https://dx.doi.org/10.1109/MC.2014.296" typeof="schema:Article">Colpaert, P., Van Compernolle, M., De Vocht, L., Dimou, A., Vander Sande, M., Mechant, P., Verborgh, R., Mannens, E.: Quantifying the Interoperability of Open Government Datasets. Computer. 47, 50–56 (2014).</dd>
  <dt id="ref-3">[3]</dt>
  <dd resource="#rezaei2014review" typeof="schema:Article">Rezaei, R., Chiew, T.K., Lee, S.P.: A review on E-business Interoperability Frameworks. Journal of Systems and Software. 93, 199–216 (2014).</dd>
  <dt id="ref-4">[4]</dt>
  <dd resource="#lubbers2016html5" typeof="schema:Article">Lubbers, P., Greco, F.: Html5 websocket: A quantum leap in scalability for the web (2010). URL <a href="http://www">http:/​/​www</a>. websocket. org/quantum. html. (2016).</dd>
  <dt id="ref-5">[5]</dt>
  <dd resource="https://dx.doi.org/10.1109/MIC.2012.64" typeof="schema:Article">Pimentel, V., Nickerson, B.G.: Communicating and Displaying Real-Time Data with WebSocket. IEEE Internet Computing. 16, 45–53 (2012).</dd>
  <dt id="ref-6">[6]</dt>
  <dd resource="#slodziak2016performance" typeof="schema:Article">Słodziak, W., Nowak, Z.: Performance Analysis of Web Systems Based on XMLHttpRequest, Server-Sent Events and WebSocket. In: Information Systems Architecture and Technology: Proceedings of 36th International Conference on Information Systems Architecture and Technology–ISAT 2015–Part II. pp. 71–83. Springer (2016).</dd>
  <dt id="ref-7">[7]</dt>
  <dd resource="#rojas2018preliminary" typeof="schema:Article">Rojas Meléndez, J.A., Van de Vyvere, B., Gevaert, A., Taelman, R., Colpaert, P., Verborgh, R.: A Preliminary Open Data Publishing Strategy for Live Data in Flanders. In: Companion Proceedings of the The Web Conference 2018. pp. 1847–1853. International World Wide Web Conferences Steering Committee (2018).</dd>
  <dt id="ref-8">[8]</dt>
  <dd resource="https://dx.doi.org/10.1007/978-3-319-60131-1_26" typeof="schema:Article">Colpaert, P., Verborgh, R., Mannens, E.: Public Transit Route Planning through Lightweight Linked Data Interfaces. In: Cabot, J., Virgilio, R. de, and Torlone, R. (eds.) Proceedings of the 17th International Conference on Web Engineering. pp. 403–411. Springer (2017).</dd>
  <dt id="ref-9">[9]</dt>
  <dd resource="https://dx.doi.org/10.1109/SysEng.2017.8088251" typeof="schema:Article">Naik, N.: Choice of effective messaging protocols for IoT systems: MQTT, CoAP, AMQP and HTTP. In: 2017 IEEE International Systems Engineering Symposium (ISSE). pp. 1–7 (2017).</dd>
  <dt id="ref-10">[10]</dt>
  <dd resource="https://www.w3.org/TR/websub/" typeof="schema:CreativeWork">Genestoux, J., Fitzpatrick, B., Slatkin, B., Atkins, M.: WebSub W3C Recommendation 23 January 2018. <a href="https://www.w3.org/TR/websub/">https:/​/​www.w3.org/TR/websub/</a></dd>
  <dt id="ref-11">[11]</dt>
  <dd resource="https://dx.doi.org/10.1007/978-3-030-00668-6_15" typeof="schema:Article">Taelman, R., Van Herwegen, J., Vander Sande, M., Verborgh, R.: Comunica: a Modular SPARQL Query Engine for the Web. In: Vrandečić, D., Bontcheva, K., Suárez-Figueroa, M.C., Presutti, V., Celino, I., Sabou, M., Kaffee, L.-A., and Simperl, E. (eds.) Proceedings of the 17th International Semantic Web Conference. pp. 239–255. Springer (2018).</dd>
  <dt id="ref-12">[12]</dt>
  <dd resource="#arasu2006cql" typeof="schema:Article">Arasu, A., Babu, S., Widom, J.: The CQL continuous query language: semantic foundations and query execution. The VLDB Journal. 15, 121–142 (2006).</dd>
  <dt id="ref-13">[13]</dt>
  <dd resource="#dell2014rsp" typeof="schema:Article">Dell’Aglio, D., Della Valle, E., Calbimonte, J.-P., Corcho, O.: RSP-QL semantics: A unifying query model to explain heterogeneity of RDF stream processing systems. International Journal on Semantic Web and Information Systems (IJSWIS). 10, 17–44 (2014).</dd>
  <dt id="ref-14">[14]</dt>
  <dd resource="#mauri2016triplewave" typeof="schema:Article">Mauri, A., Calbimonte, J.-P., Dell’Aglio, D., Balduini, M., Brambilla, M., Della Valle, E., Aberer, K.: Triplewave: Spreading RDF streams on the web. In: International Semantic Web Conference. pp. 140–149. Springer (2016).</dd>
  <dt id="ref-15">[15]</dt>
  <dd resource="#barbieri2010c" typeof="schema:Article">Barbieri, D.F., Braga, D., Ceri, S., VALLE, E.M.A.N.U.E.L.E.D.E.L.L.A., Grossniklaus, M.: C-SPARQL: a continuous query language for RDF data streams. International Journal of Semantic Computing. 4, 3–25 (2010).</dd>
  <dt id="ref-16">[16]</dt>
  <dd resource="#le2011native" typeof="schema:Article">Le-Phuoc, D., Dao-Tran, M., Parreira, J.X., Hauswirth, M.: A native and adaptive approach for unified processing of linked streams and linked data. In: International Semantic Web Conference. pp. 370–388. Springer (2011).</dd>
  <dt id="ref-17">[17]</dt>
  <dd resource="https://dx.doi.org/10.1007/978-3-319-47602-5_44" typeof="schema:Article">Taelman, R., Verborgh, R., Colpaert, P., Mannens, E.: Continuous Client-side Query Evaluation over Dynamic Linked Data. In: Sack, H., Rizzo, G., Steinmetz, N., Mladenić, D., Auer, S., and Lange, C. (eds.) Proceedings of the 13th Extended Semantic Web Conference: Satellite events. pp. 273–289. Springer (2016).</dd>
  <dt id="ref-18">[18]</dt>
  <dd resource="https://academic.microsoft.com/paper/2756671071" typeof="schema:Article">Dell’Aglio, D., Phuoc, D.L., Le-Tuan, A., Ali, M.I., Calbimonte, J.-P.: On a Web of Data Streams. DeSemWeb@ISWC. (2017).</dd>
  <dt id="ref-19">[19]</dt>
  <dd resource="#dell2017stream" typeof="schema:Article">Dell’Aglio, D., Della Valle, E., van Harmelen, F., Bernstein, A.: Stream reasoning: A survey and outlook. Data Science. 1, 59–83 (2017).</dd>
  <dt id="ref-20">[20]</dt>
  <dd resource="https://dx.doi.org/10.5439/1052222" typeof="schema:CreativeWork">Jager, A., D.; Andreas: NREL National Wind Technology Center (NWTC): M2 Tower (1996).</dd>
  <dt id="ref-21">[21]</dt>
  <dd resource="https://dx.doi.org/10.1145/3308558.3313685" typeof="schema:Article">Brickley, D., Burgess, M., Noy, N.: Google Dataset Search: Building a Search Engine for Datasets in an Open Web Ecosystem. In: The World Wide Web Conference. pp. 1365–1375. Association for Computing Machinery, New York, NY, USA (2019).</dd>
  <dt id="ref-22">[22]</dt>
  <dd resource="https://developers.google.com/transit/gtfs-realtime" typeof="schema:CreativeWork">Google: GTFS Realtime Overview. <a href="https://developers.google.com/transit/gtfs-realtime">https:/​/​developers.google.com/transit/gtfs-realtime</a></dd>
  <dt id="ref-23">[23]</dt>
  <dd resource="https://docs.cloudlab.us/hardware.html" typeof="schema:CreativeWork">Hardware HP ProLiant m400 server at Cloudlab Utah. <a href="https://docs.cloudlab.us/hardware.html">https:/​/​docs.cloudlab.us/hardware.html</a></dd>
  <dt id="ref-24">[24]</dt>
  <dd resource="https://www.nginx.com/blog/nginx-ssl/" typeof="schema:CreativeWork">Nginx SSL/TLS offloading. <a href="https://www.nginx.com/blog/nginx-ssl/">https:/​/​www.nginx.com/blog/nginx-ssl/</a></dd>
  <dt id="ref-25">[25]</dt>
  <dd resource="https://dx.doi.org/10.1145/3041021.3051699" typeof="schema:Article">Vandewiele, G., Colpaert, P., Janssens, O., Van Herwegen, J., Verborgh, R., Mannens, E., Ongenae, F., De Turck, F.: Predicting train occupancies based on query logs and external data sources. In: Proceedings of the 7th International Workshop on Location and the Web (2017).</dd>
</dl>
</section>
</footer>

</div>



</body>
</html>
